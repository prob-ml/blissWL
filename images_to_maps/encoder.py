from pathlib import Path
from typing import Optional

import lightning as L
import torch
from matplotlib import pyplot as plt
from torch.optim import Adam
from torch.optim.lr_scheduler import MultiStepLR
from torchmetrics import MetricCollection

from images_to_maps.convnet import MassMapNet, ScalarShearNet
from images_to_maps.variational_dist import VariationalDist


class MassMapEncoder(L.LightningModule):
    def __init__(
        self,
        n_bands: int,
        res_init: int,
        res_midpoint: int,
        res_final: int,
        ch_init: int,
        ch_max: int,
        ch_final: int,
        initial_downsample: bool,
        more_up_layers: bool,
        num_bottleneck_layers: int,
        image_normalizers: list,
        var_dist: VariationalDist,
        optimizer_params: Optional[dict],
        scheduler_params: Optional[dict],
        loss_plots_location: str,
        mode_metrics: MetricCollection,
        sample_metrics: Optional[MetricCollection],
        sample_image_renders: MetricCollection,
    ):
        super().__init__()

        self.n_bands = n_bands
        self.res_init = res_init
        self.res_midpoint = res_midpoint
        self.res_final = res_final
        self.ch_init = ch_init
        self.ch_max = ch_max
        self.ch_final = ch_final
        self.initial_downsample = initial_downsample
        self.more_up_layers = more_up_layers
        self.num_bottleneck_layers = num_bottleneck_layers
        self.image_normalizers = torch.nn.ModuleList(image_normalizers.values())
        self.var_dist = var_dist
        self.optimizer_params = optimizer_params
        self.scheduler_params = scheduler_params
        self.loss_plots_location = loss_plots_location
        self.mode_metrics = mode_metrics
        self.sample_metrics = sample_metrics
        self.sample_image_renders = sample_image_renders

        self.initialize_networks()
        self.epoch_train_losses = []
        self.current_epoch_train_loss = 0.0
        self.current_epoch_train_batches = 0
        self.current_epochs = 0
        self.epoch_val_losses = []
        self.current_epoch_val_loss = 0.0
        self.current_epoch_val_batches = 0

    def initialize_networks(self):
        ch_per_band = sum(
            inorm.num_channels_per_band() for inorm in self.image_normalizers
        )
        self.net = MassMapNet(
            n_bands=self.n_bands,
            res_init=self.res_init,
            res_midpoint=self.res_midpoint,
            res_final=self.res_final,
            ch_per_band=ch_per_band,
            ch_init=self.ch_init,
            ch_max=self.ch_max,
            ch_final=self.ch_final,
            initial_downsample=self.initial_downsample,
            more_up_layers=self.more_up_layers,
            num_bottleneck_layers=self.num_bottleneck_layers,
            n_var_params=self.var_dist.n_params_per_source,
        )

    def sample(self, batch, use_mode=True):
        input_lst = [inorm.get_input_tensor(batch) for inorm in self.image_normalizers]
        inputs = torch.cat(input_lst, dim=2)

        x_cat_marginal = self.net(inputs)
        return self.var_dist.sample(x_cat_marginal, use_mode=use_mode)

    def _compute_loss(self, batch, logging_name):
        batch_size, _, _, _ = batch["images"].shape[0:4]

        target_cat = batch["tile_catalog"]

        # multiple image normalizers
        input_lst = [inorm.get_input_tensor(batch) for inorm in self.image_normalizers]
        inputs = torch.cat(input_lst, dim=2)
        pred = {}
        pred["x_cat_marginal"] = self.net(inputs)

        loss = self.var_dist.compute_nll(pred["x_cat_marginal"], target_cat)
        loss = loss.sum() / loss.numel()

        self.log(f"{logging_name}/_loss", loss, batch_size=batch_size, sync_dist=True)

        if logging_name == "train":
            self.current_epoch_train_loss += loss.item()
            self.current_epoch_train_batches += 1
        if logging_name == "val":
            self.current_epoch_val_loss += loss.item()
            self.current_epoch_val_batches += 1

        return loss

    def configure_optimizers(self):
        """Configure optimizers for training (pytorch lightning)."""
        optimizer = Adam(self.parameters(), **self.optimizer_params)
        scheduler = MultiStepLR(optimizer, **self.scheduler_params)
        return [optimizer], [scheduler]

    def update_metrics(self, batch, batch_idx):
        target_cat = batch["tile_catalog"]

        mode_cat = self.sample(batch, use_mode=True)
        self.mode_metrics.update(target_cat, mode_cat, None)

        sample_cat_no_mode = self.sample(batch, use_mode=False)
        self.sample_metrics.update(target_cat, sample_cat_no_mode, None)

        self.sample_image_renders.update(
            target_cat,
            mode_cat,
            self.current_epoch,
            batch_idx,
        )

    def report_metrics(self, metrics, logging_name):
        computed = metrics.compute()
        for k, v in computed.items():
            if torch.is_tensor(v) and v.numel() > 1:
                for i in range(v.numel()):
                    self.log(f"{logging_name}/{k}/bin_{i}", v[i].item(), sync_dist=True)
            else:
                self.log(
                    f"{logging_name}/{k}",
                    v.item() if torch.is_tensor(v) else v,
                    sync_dist=True,
                )

        for metric_name, metric in metrics.items():
            if hasattr(metric, "plot"):
                try:
                    plot_or_none = metric.plot()
                except NotImplementedError:
                    continue
                name = f"Epoch:{self.current_epoch}"
                name = f"{name}/{logging_name} {metric_name}"
                if self.logger and plot_or_none:
                    fig, _axes = plot_or_none
                    self.logger.experiment.add_figure(name, fig)

    def training_step(self, batch, batch_idx):
        return self._compute_loss(batch, "train")

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        with torch.no_grad():
            return self.sample(batch, use_mode=True)

    def validation_step(self, batch, batch_idx):
        self._compute_loss(batch, "val")
        self.update_metrics(batch, batch_idx)

    def test_step(self, batch, batch_idx):
        self._compute_loss(batch, "test")
        self.update_metrics(batch, batch_idx)

    def on_train_epoch_end(self):
        # Compute the average loss for the epoch and reset counters
        avg_epoch_train_loss = (
            self.current_epoch_train_loss / self.current_epoch_train_batches
        )
        self.epoch_train_losses.append(avg_epoch_train_loss)
        self.current_epoch_train_loss = 0.0
        self.current_epoch_train_batches = 0
        self.current_epochs += 1
        print(
            f"Average train loss for epoch {self.current_epoch}: {avg_epoch_train_loss}",
        )

    def on_validation_epoch_end(self):
        self.report_metrics(self.mode_metrics, "val/mode")
        self.mode_metrics.reset()
        if self.sample_metrics is not None:
            self.report_metrics(self.sample_metrics, "val/sample")
            self.sample_metrics.reset()
        if self.sample_image_renders is not None:
            self.report_metrics(self.sample_image_renders, "val/image_renders")

        avg_epoch_val_loss = (
            self.current_epoch_val_loss / self.current_epoch_val_batches
        )
        self.epoch_val_losses.append(avg_epoch_val_loss)
        self.current_epoch_val_loss = 0.0
        self.current_epoch_val_batches = 0
        print(
            f"Average val loss for epoch {self.current_epoch}: {avg_epoch_val_loss}",
        )

    def on_test_epoch_end(self):
        self.report_metrics(self.mode_metrics, "test/mode")
        if self.sample_metrics is not None:
            self.report_metrics(self.sample_metrics, "test/sample")

    def on_train_end(self):
        if not Path(self.loss_plots_location).exists():
            Path(self.loss_plots_location).mkdir(parents=True)

        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 6))

        ax[0].plot(
            range(len(self.epoch_train_losses)),
            self.epoch_train_losses,
            label="Training loss",
        )
        ax[0].set_xlabel("Epochs")
        ax[0].set_ylabel("Loss")
        ax[0].set_title("Training loss by epoch")

        ax[1].plot(
            range(len(self.epoch_val_losses)),
            self.epoch_val_losses,
            label="Validation loss",
        )
        ax[1].set_xlabel("Epochs")
        ax[1].set_ylabel("Loss")
        ax[1].set_title("Validation loss by epoch")

        fig.tight_layout()
        fig.savefig(f"{self.loss_plots_location}/train_and_val_loss.png")


class ScalarShearEncoder(L.LightningModule):
    def __init__(
        self,
        n_bands: int,
        image_normalizers: list,
        var_dist: VariationalDist,
        optimizer_params: Optional[dict],
        scheduler_params: Optional[dict],
        loss_plots_location: str,
        mode_metrics: MetricCollection,
        sample_metrics: Optional[MetricCollection],
        sample_image_renders: MetricCollection,
    ):
        super().__init__()

        self.n_bands = n_bands
        self.image_normalizers = torch.nn.ModuleList(image_normalizers.values())
        self.var_dist = var_dist
        self.optimizer_params = optimizer_params
        self.scheduler_params = scheduler_params
        self.loss_plots_location = loss_plots_location
        self.mode_metrics = mode_metrics
        self.sample_metrics = sample_metrics
        self.sample_image_renders = sample_image_renders

        self.initialize_networks()
        self.epoch_train_losses = []
        self.current_epoch_train_loss = 0.0
        self.current_epoch_train_batches = 0
        self.current_epochs = 0
        self.epoch_val_losses = []
        self.current_epoch_val_loss = 0.0
        self.current_epoch_val_batches = 0

    def initialize_networks(self):
        self.net = ScalarShearNet(
            n_bands=self.n_bands,
            n_var_params=self.var_dist.n_params_per_source,
        )

    def _unflatten_tile_catalog(self, tile_catalog):
        """Reshape scalar shear values to (batch, 1, 1, dim) for variational dist."""
        unflattened = {}
        for key, value in tile_catalog.items():
            if value.dim() == 1:
                # (batch,) -> (batch, 1, 1, 1)
                unflattened[key] = value.unsqueeze(1).unsqueeze(1).unsqueeze(-1)
            else:
                # (batch, dim) -> (batch, 1, 1, dim)
                unflattened[key] = value.unsqueeze(1).unsqueeze(1)
        return unflattened

    def sample(self, batch, use_mode=True):
        input_lst = [inorm.get_input_tensor(batch) for inorm in self.image_normalizers]
        inputs = torch.cat(input_lst, dim=2)
        inputs = inputs.squeeze(2)  # (batch, n_bands, H, W) for 2D conv

        x_cat_marginal = self.net(inputs)
        return self.var_dist.sample(x_cat_marginal, use_mode=use_mode)

    def _compute_loss(self, batch, logging_name):
        batch_size, _, _, _ = batch["images"].shape[0:4]

        target_cat = self._unflatten_tile_catalog(batch["tile_catalog"])

        input_lst = [inorm.get_input_tensor(batch) for inorm in self.image_normalizers]
        inputs = torch.cat(input_lst, dim=2)
        inputs = inputs.squeeze(2)  # (batch, n_bands, H, W) for 2D conv
        pred = {}
        pred["x_cat_marginal"] = self.net(inputs)

        loss = self.var_dist.compute_nll(pred["x_cat_marginal"], target_cat)
        loss = loss.sum() / loss.numel()

        self.log(f"{logging_name}/_loss", loss, batch_size=batch_size, sync_dist=True)

        if logging_name == "train":
            self.current_epoch_train_loss += loss.item()
            self.current_epoch_train_batches += 1
        if logging_name == "val":
            self.current_epoch_val_loss += loss.item()
            self.current_epoch_val_batches += 1

        return loss

    def configure_optimizers(self):
        """Configure optimizers for training (pytorch lightning)."""
        optimizer = Adam(self.parameters(), **self.optimizer_params)
        scheduler = MultiStepLR(optimizer, **self.scheduler_params)
        return [optimizer], [scheduler]

    def update_metrics(self, batch, batch_idx):
        target_cat = self._unflatten_tile_catalog(batch["tile_catalog"])

        mode_cat = self.sample(batch, use_mode=True)
        self.mode_metrics.update(target_cat, mode_cat, None)

        sample_cat_no_mode = self.sample(batch, use_mode=False)
        self.sample_metrics.update(target_cat, sample_cat_no_mode, None)

        self.sample_image_renders.update(
            target_cat,
            mode_cat,
            self.current_epoch,
            batch_idx,
        )

    def report_metrics(self, metrics, logging_name):
        computed = metrics.compute()
        for k, v in computed.items():
            if torch.is_tensor(v) and v.numel() > 1:
                for i in range(v.numel()):
                    self.log(f"{logging_name}/{k}/bin_{i}", v[i].item(), sync_dist=True)
            else:
                self.log(
                    f"{logging_name}/{k}",
                    v.item() if torch.is_tensor(v) else v,
                    sync_dist=True,
                )

        for metric_name, metric in metrics.items():
            if hasattr(metric, "plot"):
                try:
                    plot_or_none = metric.plot()
                except NotImplementedError:
                    continue
                name = f"Epoch:{self.current_epoch}"
                name = f"{name}/{logging_name} {metric_name}"
                if self.logger and plot_or_none:
                    fig, _axes = plot_or_none
                    self.logger.experiment.add_figure(name, fig)

    def training_step(self, batch, batch_idx):
        return self._compute_loss(batch, "train")

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        with torch.no_grad():
            return self.sample(batch, use_mode=True)

    def validation_step(self, batch, batch_idx):
        self._compute_loss(batch, "val")
        self.update_metrics(batch, batch_idx)

    def test_step(self, batch, batch_idx):
        self._compute_loss(batch, "test")
        self.update_metrics(batch, batch_idx)

    def on_train_epoch_end(self):
        avg_epoch_train_loss = (
            self.current_epoch_train_loss / self.current_epoch_train_batches
        )
        self.epoch_train_losses.append(avg_epoch_train_loss)
        self.current_epoch_train_loss = 0.0
        self.current_epoch_train_batches = 0
        self.current_epochs += 1
        print(
            f"Average train loss for epoch {self.current_epoch}: {avg_epoch_train_loss}",
        )

    def on_validation_epoch_end(self):
        self.report_metrics(self.mode_metrics, "val/mode")
        self.mode_metrics.reset()
        if self.sample_metrics is not None:
            self.report_metrics(self.sample_metrics, "val/sample")
            self.sample_metrics.reset()
        if self.sample_image_renders is not None:
            self.report_metrics(self.sample_image_renders, "val/image_renders")

        avg_epoch_val_loss = (
            self.current_epoch_val_loss / self.current_epoch_val_batches
        )
        self.epoch_val_losses.append(avg_epoch_val_loss)
        self.current_epoch_val_loss = 0.0
        self.current_epoch_val_batches = 0
        print(
            f"Average val loss for epoch {self.current_epoch}: {avg_epoch_val_loss}",
        )

    def on_test_epoch_end(self):
        self.report_metrics(self.mode_metrics, "test/mode")
        if self.sample_metrics is not None:
            self.report_metrics(self.sample_metrics, "test/sample")

    def on_train_end(self):
        if not Path(self.loss_plots_location).exists():
            Path(self.loss_plots_location).mkdir(parents=True)

        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 6))

        ax[0].plot(
            range(len(self.epoch_train_losses)),
            self.epoch_train_losses,
            label="Training loss",
        )
        ax[0].set_xlabel("Epochs")
        ax[0].set_ylabel("Loss")
        ax[0].set_title("Training loss by epoch")

        ax[1].plot(
            range(len(self.epoch_val_losses)),
            self.epoch_val_losses,
            label="Validation loss",
        )
        ax[1].set_xlabel("Epochs")
        ax[1].set_ylabel("Loss")
        ax[1].set_title("Validation loss by epoch")

        fig.tight_layout()
        fig.savefig(f"{self.loss_plots_location}/train_and_val_loss.png")
